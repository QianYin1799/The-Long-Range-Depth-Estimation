# The-Long-Range-Depth-Estimation

## ğŸ“– Introduction

**Monocular depth estimation**, a cost-effective solution for reconstructing 3D structures from single 2D images, finds widespread applications in robotic navigation, UAV control, and intelligent manufacturing. 

Despite significant advancements in deep neural networks, accurately estimating depth for **distant objects** remains challenging due to insufficient global context modeling.

To address this, we introduce a novel monocular depth estimation framework featuring:
- **CNN-Transformer dual-branch encoder** for local-global feature extraction.
- A **Cross-Dimensional Feature Fusion (CSF) Module** for enhanced global depth understanding.
- In the decoder, a **Depthwise Separable Upsampling Block (DSUB)** and **Multi-Scale Self-Attention Module (MSA)** to refine upsampling and recover detailed spatial information.

**Experimental results** demonstrate superior long-range prediction accuracy on KITTI and SUN RGB-D datasets.

---

## ğŸ“Š Benchmark Performance

| Dataset      | Abs-Rel â†“ | RMSE â†“  | Notes                              |
|:-------------|:-----------|:--------|:-----------------------------------|
| **KITTI**     | **0.053**  | **2.128** | Excellent performance, especially in distant regions |
| **SUN RGB-D** | **0.140**  | **0.417** | Using NYU pre-trained weights, shows strong generalization |

---

## ğŸ“Š Method Overview

### ğŸ“Œ Architecture Overview

> ğŸ“· **[Insert high-level framework diagram here]**

*Illustration: Dual-branch Encoder (CNN + Transformer), CSF Fusion Module, DSUB & MSA-based Decoder*

---

## ğŸ“¦ Installation

### ğŸ“‹ Prerequisites

- **Linux** or **macOS** (Windows experimental support)
- Python **3.6+**
- PyTorch **1.3+**
- CUDA **9.2+** (or CUDA 9.0 if building from source)
- GCC **5+**
- MMCV

### ğŸ› ï¸ Installation Steps

Tested with:
- PyTorch 1.8.0
- CUDA 11.1
- Python 3.7
- Ubuntu 20.04

If your environment is similar, follow these steps:

```bash
conda create -n MDE python=3.7
conda activate MDE

conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge

pip install mmcv-full==1.3.13 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html

git clone https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox.git
cd Monocular-Depth-Estimation-Toolbox
pip install -e .

pip install future tensorboard
ğŸ“‚ Dataset Preparation
It is recommended to symlink your datasets to $MONOCULAR-DEPTH-ESTIMATION-TOOLBOX/data

Example Folder Structure:

css
å¤åˆ¶
ç¼–è¾‘
monocular-depth-estimation-toolbox
â”œâ”€â”€ depth
â”œâ”€â”€ tools
â”œâ”€â”€ configs
â”œâ”€â”€ splits
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ kitti
â”‚   â”‚   â”œâ”€â”€ input
â”‚   â”‚   â”‚   â”œâ”€â”€ 2011_09_26
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ gt_depth
â”‚   â”‚   â”‚   â”œâ”€â”€ 2011_09_26_drive_0001_sync
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ benchmark_test
â”‚   â”‚   â”œâ”€â”€ benchmark_cam
â”‚   â”‚   â”œâ”€â”€ split_file.txt
â”‚   â”œâ”€â”€ nyu
â”‚   â”‚   â”œâ”€â”€ basement_0001a
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ split_file.txt
â”‚   â”œâ”€â”€ SUNRGBD
â”‚   â”‚   â”œâ”€â”€ SUNRGBD
â”‚   â”‚   â”‚   â”œâ”€â”€ kv1
â”‚   â”‚   â”‚   â”œâ”€â”€ kv2
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ split_file.txt
ğŸš€ Usage
ğŸ‹ï¸â€â™‚ï¸ Training
Single / Multi-GPU Training:

bash
å¤åˆ¶
ç¼–è¾‘
bash tools/dist_train.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]
Optional Arguments:

--no-validate

--work-dir ${WORK_DIR}

--resume-from ${CHECKPOINT_FILE}

--load-from ${CHECKPOINT_FILE}

--deterministic

Example:

bash
å¤åˆ¶
ç¼–è¾‘
bash ./tools/dist_train.sh configs/depthformer/depthformer_swint_w7_nyu.py 2 --work-dir work_dirs/saves/depthformer/depthformer_swint_w7_nyu
ğŸ§ª Inference & Testing
Single-GPU Testing:

bash
å¤åˆ¶
ç¼–è¾‘
python tools/test.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] [--show]
Example:

bash
å¤åˆ¶
ç¼–è¾‘
python tools/test.py configs/depthformer/depthformer_swinl_22k_w7_nyu.py \
checkpoints/depthformer_swinl_22k_w7_nyu.pth \
--show-dir depthformer_swinl_22k_w7_nyu_results
ğŸ“Š Qualitative Results
ğŸ“· [Insert qualitative result images here]

ğŸ“– Citation
If you find this work useful in your research, please consider citing:

bibtex
å¤åˆ¶
ç¼–è¾‘
@misc{thelongrangedepth2025,
  title={The Long-Range Depth Estimation Framework},
  author={Your Name},
  year={2025},
  note={https://github.com/your-repo/The-Long-Range-Depth-Estimation}
}
ğŸ“¬ Contact
For questions and issues, feel free to open an Issue or reach out via email: your_email@example.com

ğŸ“Œ To-Do
 Code Refactoring

 Add Demonstration Videos

 Improve Documentation
